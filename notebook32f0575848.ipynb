{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":38601,"sourceType":"datasetVersion","datasetId":30279},{"sourceId":7073315,"sourceType":"datasetVersion","datasetId":4073753}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        # print(os.path.join(dirname, filename))\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-30T00:56:46.654164Z","iopub.execute_input":"2023-11-30T00:56:46.654609Z","iopub.status.idle":"2023-11-30T00:56:47.335379Z","shell.execute_reply.started":"2023-11-30T00:56:46.654562Z","shell.execute_reply":"2023-11-30T00:56:47.334502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls /kaggle/input/","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:56:47.337203Z","iopub.execute_input":"2023-11-30T00:56:47.337652Z","iopub.status.idle":"2023-11-30T00:56:48.386060Z","shell.execute_reply.started":"2023-11-30T00:56:47.337613Z","shell.execute_reply":"2023-11-30T00:56:48.384866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Импорты необходимых библиотек","metadata":{}},{"cell_type":"code","source":"!pip install torchtoolbox","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:56:48.770786Z","iopub.execute_input":"2023-11-30T00:56:48.771900Z","iopub.status.idle":"2023-11-30T00:57:01.872346Z","shell.execute_reply.started":"2023-11-30T00:56:48.771852Z","shell.execute_reply":"2023-11-30T00:57:01.870998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.augmentations.dropout.cutout import Cutout\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom os import listdir\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nimport cv2\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom torchtoolbox.tools import mixup_data, mixup_criterion","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:01.874619Z","iopub.execute_input":"2023-11-30T00:57:01.874970Z","iopub.status.idle":"2023-11-30T00:57:01.884006Z","shell.execute_reply.started":"2023-11-30T00:57:01.874938Z","shell.execute_reply":"2023-11-30T00:57:01.882871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Конфиг","metadata":{}},{"cell_type":"code","source":"# девайс\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# размер изображения\nimage_size = 256\n\n# размер батча\nbatch_size = 64\n\n# трансформации для обучения и валидации\ntransforms_train = A.Compose([\n    A.Resize(image_size, image_size),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(30, p=0.3),\n    A.RandomSizedCrop((image_size - 60, image_size), image_size, image_size, p=0.3),\n    A.CoarseDropout(max_holes=5, max_height=50, max_width=50, min_holes=3, min_height=30, min_width=30),\n    A.Normalize(),\n    ToTensorV2()\n])\n\ntransforms_val = A.Compose([\n    A.Resize(image_size, image_size),\n    A.Normalize(),\n    ToTensorV2()\n])\n\nmixup = True\nalpha=0.2\n\n# используемая модель(\"resnet18\", \"resnet18_my\")\nmodel_name = \"resnet18\"\nmodel = models.resnet18()\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 256)\n# model.load_state_dict(torch.load(\"./runs/run_train_9/weights/best.pt\"))\nmodel.to(device)\n\n# количество эпох\nnum_epochs = 150\n\n# лосс\nlabel_smoothing = 0.1\n# criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n# loss = \"crossentropy\"\nloss = \"crossentropybalance\"\n# loss = \"kldiv\"  # для использования этой штуки выход модели нужно обернуть в софтмакс\n\n# оптимизатор\n# optimizer = optim.AdamW(model.parameters(), lr=1e-2)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n\n# шедулер(может быть None)  # https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling\n# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n# scheduler = None\n# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 14, 1e-5)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\nscheduler_name = \"ReduceLROnPlateau\"","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:07.202053Z","iopub.execute_input":"2023-11-30T00:57:07.202963Z","iopub.status.idle":"2023-11-30T00:57:07.423639Z","shell.execute_reply.started":"2023-11-30T00:57:07.202931Z","shell.execute_reply":"2023-11-30T00:57:07.422633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        dataloaders,\n        dataset_sizes,\n        num_epochs=25,\n        save_path=\"./\"\n        ):\n    since = time.time()\n    path_save_best_model = save_path + \"best.pt\"\n    best_acc = 0.0\n\n    val_loss_lst = []\n    train_loss_lst = []\n    val_acc_lst = []\n    lr_lst = []\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                if phase == \"train\" and mixup:\n                    inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n\n                    if phase==\"train\" and mixup:\n                        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n                    else:\n                        loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            \n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            if phase == 'train' and scheduler is not None:\n                if scheduler_name == \"ReduceLROnPlateau\":\n                    scheduler.step(epoch_loss)\n                else:\n                    scheduler.step()\n                lr_lst.append(scheduler._last_lr[0])\n\n            if phase == \"train\":\n                train_loss_lst.append(epoch_loss)\n            elif phase == \"val\":\n                val_loss_lst.append(epoch_loss)\n                val_acc_lst.append(epoch_acc.item())\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                torch.save(model.state_dict(), path_save_best_model)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    return {\n        \"train_lst\": train_loss_lst,\n        \"val_lst\": val_loss_lst,\n        \"val_acc_lst\": val_acc_lst,\n        \"lr_lst\": lr_lst\n    }","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:09.227415Z","iopub.execute_input":"2023-11-30T00:57:09.228072Z","iopub.status.idle":"2023-11-30T00:57:09.245724Z","shell.execute_reply.started":"2023-11-30T00:57:09.228030Z","shell.execute_reply":"2023-11-30T00:57:09.244544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42\ntorch.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:11.670089Z","iopub.execute_input":"2023-11-30T00:57:11.671025Z","iopub.status.idle":"2023-11-30T00:57:11.676518Z","shell.execute_reply.started":"2023-11-30T00:57:11.670990Z","shell.execute_reply":"2023-11-30T00:57:11.675186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CaltechDataset(Dataset):\n    def __init__(self, df, transforms):\n        self.df = df\n        self.transforms = transforms\n        self.idx2label = {idx: self.df[self.df.label_idx == idx].label.values[0] for idx in set(self.df.label_idx.values)}\n        self.label2idx = {label: idx for idx, label in self.idx2label.items()}\n  \n    def __getitem__(self, idx):\n        img_path = self.df.iloc[idx].path\n        label_idx = self.df.iloc[idx].label_idx\n\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        aug = self.transforms(image=img)\n        img = aug[\"image\"]\n    \n        return img, int(label_idx)\n\n    def __len__(self):\n        return len(self.df)\n\n\ndef get_caltech_dataframe(curr_path: str) -> pd.DataFrame:\n    with open(curr_path, \"r\") as f:\n        data_rows = f.readlines()\n\n    data_rows = [data_row[:-1] for data_row in data_rows]\n\n    dict_data = {\n        \"path\": [],\n        \"label_idx\": [],\n        \"label\": []\n    }\n\n    for item in data_rows:\n        curr_path, curr_label_idx, curr_label = item.split(\" \")\n        dict_data[\"path\"].append(\"/kaggle/input/caltech256/\" + curr_path)\n        dict_data[\"label_idx\"].append(curr_label_idx)\n        dict_data[\"label\"].append(curr_label)\n\n    df = pd.DataFrame(dict_data)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:13.790109Z","iopub.execute_input":"2023-11-30T00:57:13.790489Z","iopub.status.idle":"2023-11-30T00:57:13.803976Z","shell.execute_reply.started":"2023-11-30T00:57:13.790458Z","shell.execute_reply":"2023-11-30T00:57:13.802773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = get_caltech_dataframe(\"/kaggle/input/train-val-txt/train_lst.txt\")\ndf_val = get_caltech_dataframe(\"/kaggle/input/train-val-txt/val_lst.txt\")\n\n# 4. Определим обучающий и валидационный датасеты\ntrain_dataset = CaltechDataset(df_train, transforms_train)\nval_dataset = CaltechDataset(df_val, transforms_val)\nimage_datasets = {\n        \"train\": train_dataset,\n        \"val\": val_dataset,\n    }\n\ndataset_sizes = {\n        data_type: len(image_datasets[data_type]) for data_type in [\"train\", \"val\"]\n    }\n\n# 5. Определим даталоадеры\ndataloaders = {\n        data_type: DataLoader(\n            image_datasets[data_type],\n            batch_size=32,\n            shuffle=True\n        )\n        for data_type in [\"train\", \"val\"]\n    }","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:15.534485Z","iopub.execute_input":"2023-11-30T00:57:15.535475Z","iopub.status.idle":"2023-11-30T00:57:16.893482Z","shell.execute_reply.started":"2023-11-30T00:57:15.535436Z","shell.execute_reply":"2023-11-30T00:57:16.892296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Обучение модели","metadata":{}},{"cell_type":"code","source":"if loss == \"crossentropy\":\n    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\nelif loss == \"crossentropybalance\":\n    # получим веса для каждого класса\n    all_train_lbls = []\n    for item, lbl in tqdm(train_dataset):\n        all_train_lbls.append(lbl)\n    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(np.array(all_train_lbls)), y=np.array(all_train_lbls))\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean', label_smoothing=label_smoothing)\nelif loss == \"kldiv\":\n    criterion = nn.KLDivLoss()\n\n    \nres_dict = train_model(\n    model,\n    criterion,\n    optimizer,\n    scheduler,\n    dataloaders,\n    dataset_sizes,\n    num_epochs,\n    save_path=f\"./\"\n        )","metadata":{"execution":{"iopub.status.busy":"2023-11-30T00:57:16.895886Z","iopub.execute_input":"2023-11-30T00:57:16.896365Z","iopub.status.idle":"2023-11-30T09:25:05.993519Z","shell.execute_reply.started":"2023-11-30T00:57:16.896325Z","shell.execute_reply":"2023-11-30T09:25:05.992452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_df = pd.DataFrame({\n    \"train_lst\": res_dict[\"train_lst\"],\n    \"val_lst\": res_dict[\"val_lst\"],\n    \"val_acc_lst\": res_dict[\"val_acc_lst\"],\n    \"lr_lst\": res_dict[\"lr_lst\"]\n})\n\nres_df.to_csv(\"res_df.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T09:25:05.995246Z","iopub.execute_input":"2023-11-30T09:25:05.995591Z","iopub.status.idle":"2023-11-30T09:25:06.005720Z","shell.execute_reply.started":"2023-11-30T09:25:05.995551Z","shell.execute_reply":"2023-11-30T09:25:06.004613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}